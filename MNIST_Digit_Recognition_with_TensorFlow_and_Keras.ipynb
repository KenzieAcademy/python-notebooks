{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Digit Recognition with TensorFlow and Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wmQFpcHB9Pxq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti4OweEypIKR"
      },
      "source": [
        "# Digit Recognition with TensorFlow and Keras\n",
        "This notebook illustrates how to use Tensorflow and Keras to build and train a Convolutional Neural Network (CNN) to perform digit recognition.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "<img height=\"60\" src=\"https://www.tensorflow.org/images/tf_logo_social.png\" />\n",
        "\n",
        "[**TensorFlow**](https://tensorflow.org):\n",
        "> An end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n",
        "\n",
        "<img height=\"60\" src=\"https://keras.io/img/logo-k-keras-wb.png\" />\n",
        "\n",
        "[**Keras**](https://keras.io/): \n",
        "> A deep-learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFk4uwLth5g2"
      },
      "source": [
        "## The Goal\n",
        "1. We will create our own multi-layer neural network for digit recognition using Keras and TensorFlow\n",
        "2. Train the network on the MNIST training data.\n",
        "3. Use the trained network to inference new digits that are presented on the inputs.\n",
        "\n",
        "![picture](https://miro.medium.com/max/700/1*XdCMCaHPt-pqtEibUfAnNw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCD2mbwpyYM"
      },
      "source": [
        "## What is MNIST\n",
        "MNIST stands for [Modified National Institute of Standards and Technology](https://en.wikipedia.org/wiki/MNIST_database).  It is a large database of handwritten digits that is commonly used for training various image processing systems  The handwritten digits look like this\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "The dataset is a blend of digit images taken from handwritten notes from Census Bureau employees and high school students.  It is one of the most common datasets used for image classification and it is accessible from many different sources. Tensorflow and Keras allow direct imports of this dataset from their API.\n",
        "\n",
        "The MNIST database contains 60,000 training images and 10,000 testing images. Here we separate the groups according to training vs testing. These groups are further subdivided into images and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYRM9r-SpGb7"
      },
      "source": [
        "import tensorflow as tf\n",
        "# x dimension is greyscale image data\n",
        "# y dimension is actual digit labels\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(f\"Training images: {len(x_train):10}\")\n",
        "print(f\"Training labels: {len(y_train):10}\")\n",
        "print(f\"Testing images:  {len(x_test):10}\")\n",
        "print(f\"Testing labels:  {len(y_test):10}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqSeoeQ8DhF5"
      },
      "source": [
        "## View a random image from the dataset.\n",
        "\n",
        "Note the dimensions of each image: 28 x 28 x 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEA6cFoBpQpN"
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "index = random.randint(0, len(x_train)-1)\n",
        "print(f'The selected index is {index}')\n",
        "print(f'The label of this data is {y_train[index]}')\n",
        "plt.imshow(x_train[index], cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnmRCNrIHynU"
      },
      "source": [
        "## What is the \"shape\" of the data?\n",
        "The x and y datasets are represented by a `numpy.ndarray` object.  An `ndarray` is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers that specify the sizes of each dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xH8HZ9H_H8"
      },
      "source": [
        "print(f\"Type of data: {type(x_train)}\")\n",
        "print(f\"Shape of data: {x_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCf_bfjCJiDU"
      },
      "source": [
        "Before using Keras, we need to re-shape the input data into 4 dimensions instead of 3.  The four dimensions are:\n",
        "- number of images (60000)\n",
        "- height of each image (28)\n",
        "- width of each image (28)\n",
        "- depth of each image (1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-3oOIqJs-u"
      },
      "source": [
        "# Reshape all input image data (x) into 4 dimensions\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Convert input pixel data to float instead of int, because we'll be dividing\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the Greyscale values by dividing by max Greyscale value.  \n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print(f\"New shape of x_train: {x_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylz58t3soW9O"
      },
      "source": [
        "## Building the Convolutional Neural Network (CNN)\n",
        "We will create a CNN in Keras, using a Sequential model and adding layers to it:\n",
        "\n",
        "1. 2D Convolution\n",
        "2. 2D Maxpool\n",
        "3. Flatten\n",
        "4. Dense with RELU activation\n",
        "5. Dropout\n",
        "6. Dense with Softmax activation\n",
        "\n",
        "The model will look something like this\n",
        "\n",
        "<img height=\"300\" src=\"https://cdn-images-1.medium.com/max/628/1*RM7nqjYSMxkc0QlCxERQnw.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1irqy3AIWuwJ"
      },
      "source": [
        "\n",
        "### 2D Convolution\n",
        "This first network layer is used to reduce computational complexity, while still preserving ability to detect features in the images. Notice the 3x3 matrix that is sliding over the larger 5x5 matrix.  This is called a `kernel` and performs a weighted sum of the cells in it's view as it slides over the large matrix.\n",
        "\n",
        "<img height=\"200\" src=\"https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" />\n",
        "\n",
        "Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKlUWn_rgFMW"
      },
      "source": [
        "### 2D Maxpool\n",
        "<img height=\"200\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png\" />\n",
        "\n",
        "Why this layer?\n",
        "- Dimension reduction. Reduces the number of parameters to learn and the amount of computation performed in the network.\n",
        "- The pooling layer summarizes the features present in a region of the feature map generated by a convolution layer. So, further operations are performed on summarized features instead of precisely positioned features generated by the convolution layer. This makes the model more robust to variations in the position of the features in the input image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-LV_joXkkcq"
      },
      "source": [
        "### [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/)\n",
        "<img height=\"280\" src=\"https://miro.medium.com/max/700/1*IWUxuBpqn2VuV-7Ubr01ng.png\" />\n",
        "\n",
        "Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer. In other words, we put all the pixel data in one line and make connections with the final layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQuU45E6pZDq"
      },
      "source": [
        "### [Dense + RELU](https://keras.io/api/layers/core_layers/dense/)\n",
        "After the flatten, we have a feature vector with 128 elements.  The Dense layer performs the simple neuron function of\n",
        "\n",
        "    output = (input * weight) + bias\n",
        "\n",
        "and then passes the output through an activation function called RELU (Rectified Linear Unit activation) which is basically just a max() function.\n",
        "\n",
        "<img height=\"240\" src=\"https://miro.medium.com/max/746/1*umurYoig4DJ2_j3AIjI4Gw.png\" />\n",
        "\n",
        "This activation function will only pass values that are greater than 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Mh-vbpsrr3"
      },
      "source": [
        "### [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
        "The Dropout layer randomly sets a certain percentage of input units to 0.0 at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - drop_rate) such that the sum over all inputs is unchanged.  Note that the Dropout layer only applies when the model is being trained, not when it is used for predicting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ti2MBexBQC"
      },
      "source": [
        "### [Dense + Softmax](https://keras.io/api/layers/activation_layers/softmax/)\n",
        "<img height=\"280\" src=\"https://krisbolton.com/images/posts/2018/softmax-activation-function.jpg\" />\n",
        "\n",
        "This is the output layer of 10 outputs, one for each digit.  We need a function that takes input values and transforms them into a probability distribution.  The function is great for classification problems, especially if you’re dealing with multi-class classification problems, as it will report back the “confidence score” for each class. Since we’re dealing with probabilities here, the scores returned by the softmax function will add up to 1.\n",
        "The predicted class is, therefore, the item in the list where confidence score is the highest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q7n55pHNTbF"
      },
      "source": [
        "# We will use the Sequential model from Keras, which allows to manually build the layers.\n",
        "from tensorflow.keras.models import Sequential\n",
        "# These are the layers that will be in the model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "\n",
        "# Construct the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(Dense(128, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.2))  # For training only: randomly zeros 20% of inputs\n",
        "model.add(Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "# Let's see the summary\n",
        "print(f\"The model output shape is {model.output_shape}\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh-8eK8dW7R6"
      },
      "source": [
        "## Compile and Train the Model\n",
        "Now that the model (Neural Network) is defined, it must be compiled.  Compiling assigns a specific loss function, as well as an optimizer method which instructs how to perform the training.\n",
        "\n",
        "For this model we chose the \"Adam\" (Adaptive Momentum) optimizer instead of a standard gradient descent.  Adam gets the speed from momentum and the ability to adapt to gradients in different directions from RMSProp. The combination of the two makes it powerful.\n",
        "\n",
        "### Gradient Descent\n",
        "This is how models are trained.  Think of each ball as a different method of finding the lowest spot on the feature map.  Finding the lowest spot means that the network has found the best combinations of all weights to minimze it's predictive errors.\n",
        "\n",
        "<img height=\"300\" src=\"https://miro.medium.com/max/1432/1*47skUygd3tWf3yB9A10QHg.gif\" />\n",
        "\n",
        "### Selecting the Loss Function\n",
        "The purpose of loss (error) functions is to compute the quantity that a model should seek to minimize during training.  Since all our images are labeled, we select a type of loss function called \"Probabilistic\" because our final 10 digit categories are represented by probability scores.  The loss function is selected to be \"sparse categorical crossentropy\" because our digit outputs are mutually exclusive (e.g. each sample belongs exactly to one class).  A 1 can never also be a 2, a 2 cannot ever also be a 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0rTI_9AA7X5"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam', # Extends the SDG optimizer\n",
        "    loss='sparse_categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "model.fit(x=x_train, y=y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lErNUsQ8jJG"
      },
      "source": [
        "## Check the network\n",
        "In this stage we ask, how does our model perform against the actual test data, which it has never seen during the training phase?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI0YBss4CxjM"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA9PqTb7828t"
      },
      "source": [
        "## Use the network to make a prediction!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRZKUPEhDvY0"
      },
      "source": [
        "# pick a number from 0 - 9999\n",
        "index = 874\n",
        "\n",
        "# retrieve the 28 x 28 image data\n",
        "img = x_test[index]\n",
        "\n",
        "# give the image to the model. Needs a slight reshape first.\n",
        "pred = model.predict(img.reshape(1, 28, 28, 1))\n",
        "\n",
        "print(f\"Predicted Digit: {pred.argmax()}\")\n",
        "print(\"\\nDigit  Probabilities\")\n",
        "for i, p in enumerate(pred[0]):\n",
        "    print(f\"{i}:     {p:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlnAHAenGwIs"
      },
      "source": [
        "# Let's verify our model's prediction\n",
        "plt.imshow(img.reshape(28, 28), cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmQFpcHB9Pxq"
      },
      "source": [
        "## Conclusion\n",
        "- Keras makes machine learning highly accessible to developers\n",
        "- Sample code to try out MNIST dataset is everywhere. \n",
        "- There are many variations of CNN that can perform with similar accuracy\n",
        "- Big $$$ in designing predictive models that are accurate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouX2qYQ6DV6C"
      },
      "source": [
        "## References\n",
        "- [Keras Documentation](https://keras.io/about/)\n",
        "- [Keras Simple MNIST convnet](https://keras.io/examples/vision/mnist_convnet/)\n",
        "- [Visualizing Gradient Descent Methods](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
        "- [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "- [Image classification with MNIST](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)\n",
        "- [Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FlmmSRKWKCX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}